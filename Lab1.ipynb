{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bUC50ibj301R"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/alexwolson/LabattCARTEBootcamp/blob/master/Lab1.ipynb)\n",
    "\n",
    "# UofT CARTE Labatt ML Bootcamp\n",
    "#### Lab 1\n",
    "##### Lab author: Alexander Olson, alex.olson@utoronto.ca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tULCi68H301T"
   },
   "source": [
    "In this lab, we will extend our work with Scikit-Learn from the introductory lab, looking at K-Nearest Neighbours and a range of linear classifiers. Finally, we will look at linear regression, and introduce the concept of regularizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gHxml7ir301V"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "# use seaborn plotting defaults\n",
    "import seaborn as sns; sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BJhmTOhI301W"
   },
   "source": [
    "### Creating a k-Nearest Neighbors Classifier\n",
    "\n",
    "Continuing on from the introductory lab, we are working using the toy \"iris\" dataset from scikit-learn. We've explored the data a little bit, and now we're going to use scikit-learn to create a k-nearest neighbors classifier for the data. Effectively we'll be developing a model whose job it is to build a relationship over input feature data (sepal and petal characteristics) that predicts the iris sample class (e.g. \"setosa\"). This is an example of a *supervised learning* task; we have all the features and all the target classes.\n",
    "\n",
    "Nearest neightbors classifiers are quite simple. They predict the class of a new data sample based off the *nearest* data points to that sample. The 'nearest' metric is calculated via a distance function (often [Euclidean distance](https://en.wikipedia.org/wiki/Euclidean_distance)). \n",
    "\n",
    "<img src=\"https://github.com/alexwolson/LabattCARTEBootcamp/blob/master/img/knn.png?raw=1\" alt=\"knn\" width=\"200\"/>\n",
    "\n",
    "For example, in the above diagram, suppose we are looking to classify the green circle as either a red triangle, or a blue square. If k = 1 (i.e., we look at one neighbor), our model would predict *red triangle*. If k=2, it would still predict *red triangle*. If k=3, the model would predict *red triangle* as it is the *majority* class of the 3 nearest neighbors. It isn't until k=5 that the algorithm actually predicts *blue square*.\n",
    "\n",
    "Model creation in scikit-learn follows a **data prep -> fit -> predict** process. The \"fit\" function is where the actual model is trained and parameter values are selected, while the \"predict\" function actually takes the trained model and applies it to the new samples.\n",
    "\n",
    "First, we load the nearest neighbor library from scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QjsTRC6j301X"
   },
   "outputs": [],
   "source": [
    "from sklearn import neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LLkh9234301Y"
   },
   "source": [
    "Now, we're going to save our feature data into an array called 'X' and our target data into an array called 'y'. We don't *need* to do this, but it is traditional to think of the problem using this notation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y5Pcw9-b301Y"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris_data = load_iris()\n",
    "X = iris_data.data\n",
    "y = iris_data.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "buM1cY73301Z"
   },
   "source": [
    "Next, we create our nearest neighbor classifier object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MtYxPXh3301a"
   },
   "outputs": [],
   "source": [
    "knn = neighbors.KNeighborsClassifier(n_neighbors=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vM52SSUI301a"
   },
   "source": [
    "And then we *fit* it to the data (i.e., train the classifier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fx0rpW_J301b",
    "outputId": "ac56cc4e-1397-48e4-fa27-a849781e4ac3"
   },
   "outputs": [],
   "source": [
    "knn.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J4fLXBGC301c"
   },
   "source": [
    "Now we have a model! If you're new to this, you've officially built your first machine learning model. If you use \"knn.predict(*[[feature array here]]*)\", you can use your trained model to predict the class of a new iris sample. \n",
    "\n",
    "**YOUR TURN:**\n",
    "* What is the predicted class of a new iris sample with feature vector [3,4,5,2]? What is its name? ________________\n",
    "* Do you think this model is overfit or underfit to the iris dataset? Why? ________________\n",
    "* How many neighbors does our model consider when classifying a new sample? ________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CWBnd5df301c"
   },
   "source": [
    "As you may have noted in the previous cell, we've trained this classifier on our *entire dataset*. This typically isn't done in practice and results in overfitting to the data. Here's a bit of a tricky question:\n",
    "\n",
    "**YOUR TURN:**\n",
    "* If we use our classifier to predict the classes of the iris samples that were used to train the model itself, what will our overall accuracy be? ________________\n",
    "\n",
    "We can validate our hypothesis fairly easily using either: i) the NumPy technique for calculating accuracy we used earlier in the lab, or ii) scikit-learn's in-house \"accuracy_score()\" function.\n",
    "\n",
    "Let's use our technique first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eN6dB4nB301d",
    "outputId": "298779e7-161b-4bdb-9912-29a02f1d9f80"
   },
   "outputs": [],
   "source": [
    "accuracy = np.sum(y == knn.predict(X)) / y.size\n",
    "print (\"Accuracy: \", accuracy * 100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1dd_idIb301d"
   },
   "source": [
    "and then using scikit-learn's customized function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8wWeh2sO301d",
    "outputId": "ab59c107-beaf-42cb-9e30-0b9e135184bd"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y, knn.predict(X))\n",
    "print (\"Accuracy: \", accuracy * 100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6PGX_bvq301e"
   },
   "source": [
    "We see that our classifier has achieved 100% accuracy (and both calculation methods agree)!\n",
    "\n",
    "**DISCUSSION:** \n",
    "* Why do you think the model was able to achieve such a \"great\" result? ______________________\n",
    "* What does this really tell us?  __________________________________\n",
    "* Do you expect the model to perform this well on new data? __________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fLCLlB-B301f"
   },
   "source": [
    "### Cross Validation\n",
    "\n",
    "A popular way to mitigate this overfitting issue is to train your model on *some* of the data (the training set) and validate your model on the remaining data (the validation set). You will then select the model/configuration that performs best on the validation data. The train/validate division of the data is usually done with a 70%/30% split. Often, practitioners will use a third data set, the test set (or hold-out set), to get a sense for how their best model performs on unseen, real-world data. In this scenario, you will tune your models to perform best on the validation set and then test their \"real-world\" performance on the unseen test set.\n",
    "\n",
    "Sometimes applications don't have enough data to do these splits meaningfully (e.g., the test data is only a few samples). In these cases, *cross-validation* is a useful technique (and, indeed, has become standard in machine learning practice). \n",
    "\n",
    "The general premise of \"k-folds\" cross validation is to first divide the entire dataset (grey) into a training set (green) and a test set (unseen data, blue). Then, we divide the training set into different folds and use these folds to form new sub-training and sub-test sets. We select the model configuration that performs the best on all of these. The below figure provides a nice visualization for what's going on here:\n",
    "\n",
    "<img src=\"https://github.com/alexwolson/LabattCARTEBootcamp/blob/master/img/cross-val.png?raw=1\" alt=\"cross-val\" width=\"500\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tcPJ6Ri5301f"
   },
   "source": [
    "Accomplishing k-folds cross validation in scikit-learn is a manageable task. First, we divide our data into a train and test set, then we conduct the cross validation and look at the mean scores across the splits, then we conduct our final evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JLlE53c6301f"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wSUxlQ8P301f"
   },
   "source": [
    "We have divided our data into two sections: training data (70% of the data) and testing data (30% of the data). Now we will fit our nearest neighbors classifier to the training data with 5 folds and see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IA97TpsL301g",
    "outputId": "15dcd860-7a43-40fc-ed15-4babc15aab34"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(knn, X_train, y_train, cv=5)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bBIboJvg301g"
   },
   "source": [
    "Our cross-validated model has an accuracy of 94% across all the splits on the training data. If we think that is a reasonable value, we can train our final model on the training data and then see how it performs on the held-out test data. \n",
    "\n",
    "##### Comparing classifiers\n",
    "However, to get a true sense for the utility of cross-validation, let's create a second nearest neighbors classifier that uses two neighbors instead of one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tWNw4ul3301h",
    "outputId": "820708aa-c59c-4344-b17f-9b0e4898826b"
   },
   "outputs": [],
   "source": [
    "knn2 = neighbors.KNeighborsClassifier(n_neighbors=2)\n",
    "scores = cross_val_score(knn2, X_train, y_train, cv=5)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EclOqyA2301h"
   },
   "source": [
    "As we see above, our second classifier (the one with two neighbors) actually performs worse when cross-validated (92% vs. 94% mean accuracy on the 5 folds)! So, we'll stick with the first one. \n",
    "\n",
    "Let's train it on the training data and use it to predict the final held-out test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y2yZE1X1301h",
    "outputId": "682d3a56-b8cb-46ba-8265-696a53032a5d"
   },
   "outputs": [],
   "source": [
    "knn.fit(X_train, y_train)\n",
    "accuracy = accuracy_score(y_test, knn.predict(X_test))\n",
    "print (\"Test set accuracy: \", accuracy * 100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0jdgoDQg301h"
   },
   "source": [
    "And we see our model has a 97.7% accuracy on the held out test data (30% of the original dataset)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HIJRcdRV301i"
   },
   "source": [
    "# Regression: Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b49hBHoG301i"
   },
   "source": [
    "The models we have looked at so far all fall under the umbrella term of _classification_. That is to say, these models take in some input data and return the predicted _class_ that data falls under. But there is another core type of model which we are yet to look at: _regression_ models.\n",
    "\n",
    "Regression models take in some input value X, but instead of returning an expected class they output another numerical value. This is important because in some problems we don't want to limit ourselves to _discrete_ classes - think about the difference between a model which predicts a person's age, and a model which predicts the generation someone was born in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ovucBq6q301i"
   },
   "source": [
    "**YOUR TURN**\n",
    "\n",
    "* What is an example of a problem that should be solved using a _classifier_? __________\n",
    "* What is an example of a problem that should be solved using a _regressor_? __________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6nr6nb0C301j"
   },
   "source": [
    "# Regression: Ordinary Least Squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UJEV8xqD301j"
   },
   "source": [
    "One of the simplest regression models we will look at is Ordinary Least Squares (OLS), a type of linear regressor. OLS fits a linear model with coefficients $w = (w_1..w_p)$ to minimize the _residual sum of squares_ between the observed targets in the dataset, and the targets predicted by the regressor. That is to say, the objective of the model is to minimize the difference between the predicted and actual value for each training point, squared. This is represented mathematically by the _cost function_ below: $$min_w||Xw-y||^2_2$$\n",
    "\n",
    "The subscript 2 refers to the fact that this computation is using L2 norm - for now, we will skip this, but it essentially refers to calculating the Euclidean distance between $Xw$ and $y$. The superscript 2 refers to the fact that this cost term is squared."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rxxbpK00301j"
   },
   "source": [
    "Let's now take two loosely correlated features from the wine dataset and plot them. We will use alcohol level and color intensity for our first regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XtmQZIz7301k"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "wine_data = load_wine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "32mgXmVF301k"
   },
   "outputs": [],
   "source": [
    "alcohol = wine_data.data[:,wine_data.feature_names.index('alcohol')]\n",
    "color = wine_data.data[:,wine_data.feature_names.index('color_intensity')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "id": "FdYjkPbE301k",
    "outputId": "c11a0716-f270-4fed-cc46-2320a7cd37aa"
   },
   "outputs": [],
   "source": [
    "plt.scatter(alcohol,color)\n",
    "plt.xlabel('Alcohol level (%)')\n",
    "plt.ylabel('Color intensity');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M8JfhIKD301l"
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "reg = linear_model.LinearRegression() #intialize\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(alcohol.reshape(-1,1), #Have to reshape because we only have one feature\n",
    "                                                 color,\n",
    "                                                 test_size=0.3,\n",
    "                                                 random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oroun3ge301l"
   },
   "outputs": [],
   "source": [
    "reg.fit(X_train,y_train); # fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "id": "nowuf7FG301l",
    "outputId": "03dec9a2-e51a-417f-8a93-ae68a4f8e8e6"
   },
   "outputs": [],
   "source": [
    "plt.scatter(alcohol,color)\n",
    "plt.xlabel('Alcohol level (%)')\n",
    "plt.ylabel('Color intensity');\n",
    "plt.plot(alcohol, [(t * reg.coef_[0]) + reg.intercept_ for t in alcohol],c='red');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xo0i4VDx301l"
   },
   "source": [
    "The red line here represents the prediction our model makes at each alcohol level. As you can see, although it does follow the general trend of the data, some of the data points are very far off the line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3V7io_bi301m"
   },
   "source": [
    "Although it is technically possible to use our previous scoring methods to calculate the performance of this model, those scoring methods are designed for classification tasks and not for regression. Here, we should score our model using metrics better suited to the type of task. We will look at two: Mean Squared Error, and R-squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3hqV9yZE301m"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qfBtpIyF301m",
    "outputId": "701c214a-245f-4713-e7cd-5f0120f9a4ce"
   },
   "outputs": [],
   "source": [
    "print(\"Mean Squared Error (MSE):\\t%.2f\" % (mean_squared_error(y_test, reg.predict(X_test))))\n",
    "print(\"R-Squared Score:\\t\\t%.2f\" % (r2_score(y_test, reg.predict(X_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BW6kEzSQ301m"
   },
   "source": [
    "R-Squared and MSE report two different ways of looking at the model's error. MSE is quite close to the loss function that OLS uses to fit the model in the first place - it calculates the average squared difference between the predicted value of the model, and the true target value. R-Squared, on the other hand, represents how closely the data conforms to the line we have calculated. \n",
    "\n",
    "Now that we have an intuition for how OLS works, let's expand from two dimensions to the full 13 dimensions available in the wine dataset. We will use 12 of the dimensions to predict one of the dimensions: alcohol percentage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZdowoxHz301n"
   },
   "source": [
    "**YOUR TURN** Initialise a Linear Regression model, as above, fit the model to the 12 dimensions which are not `alcohol`, and then calculate the mean squared error and R-squared score on a testing set.\n",
    "\n",
    "* What was the MSE? ______\n",
    "* What was the R-Squared Score? ______\n",
    "* Did the model perform well or poorly? Why? ______"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Lab1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
